{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44b18d08",
   "metadata": {},
   "source": [
    "### Terminology\n",
    "S: this is a matrix that represents Private Key needed to decrypt the data.\n",
    "\n",
    "M: This is a public key needed to encrypt the data and perform math operations. \n",
    "\n",
    "c: This vector is an encrypted data.\n",
    "\n",
    "x: This represents review/plaintext/message.\n",
    "\n",
    "w: This is a single \"weighting\" scalar variable which will be used to re-weight our input message x (make it consistently bigger or smaller). This variable will help to tune the signal/noise ratio. Making the signal \"bigger\" makes it less susceptible to noise at any given operation. However, making it too big increases our likelihood of corrupting our data entirely. It's a balance.\n",
    "\n",
    "E or e: It refers to random noise. In some cases, this refers to noise added to the data before encrypting it with the public key. This noise is generally what makes the decryption difficult. It's what allows two encryptions of the same message to be different, which is important to make the message hard to crack. Note, this can be a vector or a matrix depending on the algorithm and implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38949eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d70c1e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keySwitch(M,c,l):\n",
    "    c_star = getBitVector(c,l)\n",
    "    return M.dot(c_star)\n",
    "\n",
    "def getRandomMatrix(row,col,bound):\n",
    "    A = np.zeros((row,col))\n",
    "    for i in range(row):\n",
    "        for j in range(col):\n",
    "            A[i][j] = np.random.randint(bound)\n",
    "    return A\n",
    "\n",
    "def getBitMatrix(S,l):\n",
    "    S_star = list()\n",
    "    for i in range(l):\n",
    "        S_star.append(S*2**(l-i-1))\n",
    "    S_star = np.array(S_star).transpose(1,2,0).reshape(len(S),len(S[0])*l)\n",
    "    return S_star\n",
    "\n",
    "def getSecretKey(T):\n",
    "    assert(T.ndim == 2)\n",
    "    I = np.eye(len(T)) # num rows\n",
    "    return hCat(I,T)\n",
    "\n",
    "def hCat(A,B):\n",
    "    return np.concatenate((A,B),1)\n",
    "\n",
    "def vCat(A,B):\n",
    "    return np.concatenate((A,B),0)\n",
    "\n",
    "def keySwitchMatrix(S, T,l):\n",
    "    S_star = getBitMatrix(S,l)\n",
    "    A = getRandomMatrix(T.shape[1],S_star.shape[1], aBound)\n",
    "    E = getRandomMatrix(S_star.shape[0], S_star.shape[1], eBound)\n",
    "    return vCat(S_star + E - T.dot(A), A)\n",
    "\n",
    "def encrypt(T, x,w,l):\n",
    "    return keySwitch(keySwitchMatrix(np.eye(len(x)),T,l), w * x,l)\n",
    "\n",
    "def addVectors(c1, c2):\n",
    "    return c1 + c2\n",
    "\n",
    "def linearTransform(M, c, l):\n",
    "    return M.dot(getBitVector(c, l)).astype('int64')\n",
    "\n",
    "def linearTransformClient(G, S, T, l):\n",
    "    return keySwitchMatrix(G.dot(S), T,l)\n",
    "\n",
    "def vectorize(M):\n",
    "    ans = np.zeros((len(M) * len(M[0]),1))\n",
    "    for i in range(len(M)):\n",
    "        for j in range(len(M[0])):\n",
    "            ans[i * len(M[0]) + j][0] = M[i][j]\n",
    "    return ans\n",
    "\n",
    "def decrypt(S, c,w):\n",
    "    Sc = S.dot(c)\n",
    "    return (Sc / w).astype('float').round().astype('int')\n",
    "\n",
    "def innerProdClient(T,l):\n",
    "    S = getSecretKey(T)\n",
    "    tvsts = vectorize(S.T.dot(S)).T\n",
    "    mvsts = copyRows(tvsts, len(T))\n",
    "    return keySwitchMatrix(mvsts,T,l)\n",
    "\n",
    "def copyRows(row, numrows):\n",
    "    ans = np.zeros((numrows, len(row[0])))\n",
    "    for i in range(len(ans)):\n",
    "        for j in range(len(ans[0])):\n",
    "            ans[i][j] = row[0][j]\n",
    "            \n",
    "    return ans\n",
    "\n",
    "def innerProd(c1, c2, M,l):\n",
    "    \n",
    "    cc1 = np.zeros((len(c1),1))\n",
    "    for i in range(len(c1)):\n",
    "        cc1[i][0] = c1[i]\n",
    "    \n",
    "    cc2 = np.zeros((1, len(c2)))\n",
    "    for i in range(len(c2)):\n",
    "        cc2[0][i] = c2[i]\n",
    "        \n",
    "    cc = vectorize(cc1.dot(cc2))\n",
    "    \n",
    "    bv = getBitVector((cc / w).round().astype('int64'),l)\n",
    "    \n",
    "    return M.dot(bv)\n",
    "\n",
    "def one_way_encrypt_vector(vector,scaling_factor = 1000):\n",
    "    padded_vector = np.random.rand(len(vector)+1)\n",
    "    padded_vector[0:len(vector)] = vector\n",
    "    \n",
    "    vec_len = len(padded_vector)\n",
    "    \n",
    "    M_temp = (M_keys[vec_len-2].T*padded_vector*scaling_factor / (vec_len-1)).T\n",
    "    e_vector = innerProd(c_ones[vec_len-2],c_ones[vec_len-2],M_temp,l)\n",
    "    return e_vector.astype('int')\n",
    "\n",
    "def load_linear_transformation(syn0_text,scaling_factor = 1000):\n",
    "    syn0_text *= scaling_factor\n",
    "    return linearTransformClient(syn0_text.T,getSecretKey(T),T,l)\n",
    "\n",
    "def s_decrypt(vec):\n",
    "    return decrypt(getSecretKey(T_keys[len(vec)-2]),vec,w)\n",
    "\n",
    "def add_vectors(x,y,scaling_factor = 10000):\n",
    "    return x + y\n",
    "\n",
    "def transpose(syn1):\n",
    "\n",
    "    rows = len(syn1)\n",
    "    cols = len(syn1[0]) - 1\n",
    "    \n",
    "    max_rc = max(rows,cols)\n",
    "    \n",
    "    syn1_c = list()\n",
    "    for i in range(len(syn1)):\n",
    "        tmp = np.zeros(max_rc+1)\n",
    "        tmp[:len(syn1[i])] = syn1[i]\n",
    "        syn1_c.append(tmp)\n",
    "    \n",
    "    syn1_c_transposed = list()\n",
    "    \n",
    "    for row_i in range(cols):\n",
    "        syn1t_column = innerProd(syn1_c[0],v_onehot[max_rc-1][row_i],M_onehot[max_rc-1][0],l) / scaling_factor\n",
    "        for col_i in range(rows-1):\n",
    "            syn1t_column += innerProd(syn1_c[col_i+1],v_onehot[max_rc-1][row_i],M_onehot[max_rc-1][col_i+1],l) / scaling_factor\n",
    "\n",
    "        syn1_c_transposed.append(syn1t_column[0:rows+1])\n",
    "    \n",
    "    return syn1_c_transposed\n",
    "\n",
    "def int2bin(x):\n",
    "    s = list()\n",
    "    mod = 2\n",
    "    while(x > 0):\n",
    "        s.append(int(x % 2))\n",
    "        x = int(x / 2)\n",
    "    return np.array(list(reversed(s))).astype('int64')\n",
    "\n",
    "\n",
    "def getBitVector(c,l):\n",
    "    m = len(c)\n",
    "    c_star = np.zeros(l * m,dtype='int64')\n",
    "    for i in range(m):\n",
    "        local_c = int(c[i])\n",
    "        if(local_c < 0):\n",
    "            local_c = -local_c\n",
    "        b = int2bin(local_c)\n",
    "        if(c[i] < 0):\n",
    "            b *= -1\n",
    "        if(c[i] == 0):\n",
    "            b *= 0\n",
    "#         try:\n",
    "        c_star[(i * l) + (l-len(b)): (i+1) * l] += b\n",
    "#         except:\n",
    "#             print(len(b))\n",
    "#             print(i)\n",
    "#             print(len(c_star[(i * l) + (l-len(b)): (i+1) * l]))\n",
    "    return c_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f229feac",
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "Python int too large to convert to C long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-fda47368aba3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0meye\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0meyes_txt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0meyes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mone_way_encrypt_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscaling_factor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[0mv_onehot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meyes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0monehot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meyes_txt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-e5ece824f8b4>\u001b[0m in \u001b[0;36mone_way_encrypt_vector\u001b[1;34m(vector, scaling_factor)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[0mM_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mM_keys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvec_len\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpadded_vector\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscaling_factor\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvec_len\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[0me_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minnerProd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_ones\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvec_len\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc_ones\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvec_len\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mM_temp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0me_vector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'int'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_linear_transformation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msyn0_text\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscaling_factor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOverflowError\u001b[0m: Python int too large to convert to C long"
     ]
    }
   ],
   "source": [
    "# HAPPENS ON SECURE SERVER\n",
    "\n",
    "l = 100\n",
    "w = 2 ** 25\n",
    "\n",
    "aBound = 10\n",
    "tBound = 10\n",
    "eBound = 10\n",
    "\n",
    "max_dim = 16\n",
    "\n",
    "scaling_factor = 1000\n",
    "\n",
    "# keys\n",
    "T_keys = list()\n",
    "for i in range(max_dim):\n",
    "    T_keys.append(np.random.rand(i+1,1))\n",
    "\n",
    "# one way encryption transformation\n",
    "M_keys = list()\n",
    "for i in range(max_dim):\n",
    "    M_keys.append(innerProdClient(T_keys[i],l))\n",
    "\n",
    "M_onehot = list()\n",
    "for h in range(max_dim):\n",
    "    i = h+1\n",
    "    buffered_eyes = list()\n",
    "    for row in np.eye(i+1):\n",
    "        buffer = np.ones(i+1)\n",
    "        buffer[0:i+1] = row\n",
    "        buffered_eyes.append((M_keys[i-1].T * buffer).T)\n",
    "    M_onehot.append(buffered_eyes)\n",
    "    \n",
    "c_ones = list()\n",
    "for i in range(max_dim):\n",
    "    c_ones.append(encrypt(T_keys[i],np.ones(i+1), w, l).astype('int'))\n",
    "    \n",
    "v_onehot = list()\n",
    "onehot = list()\n",
    "for i in range(max_dim):\n",
    "    eyes = list()\n",
    "    eyes_txt = list()\n",
    "    for eye in np.eye(i+1):\n",
    "        eyes_txt.append(eye)\n",
    "        eyes.append(one_way_encrypt_vector(eye,scaling_factor))\n",
    "    v_onehot.append(eyes)\n",
    "    onehot.append(eyes_txt)\n",
    "\n",
    "H_sigmoid_txt = np.zeros((5,5))\n",
    "\n",
    "H_sigmoid_txt[0][0] = 0.5\n",
    "H_sigmoid_txt[0][1] = 0.25\n",
    "H_sigmoid_txt[0][2] = -1/48.0\n",
    "H_sigmoid_txt[0][3] = 1/480.0\n",
    "H_sigmoid_txt[0][4] = -17/80640.0\n",
    "\n",
    "H_sigmoid = list()\n",
    "for row in H_sigmoid_txt:\n",
    "    H_sigmoid.append(one_way_encrypt_vector(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd5da845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(layer_2_c):\n",
    "    out_rows = list()\n",
    "    for position in range(len(layer_2_c)-1):\n",
    "\n",
    "        M_position = M_onehot[len(layer_2_c)-2][0]\n",
    "\n",
    "        layer_2_index_c = innerProd(layer_2_c,v_onehot[len(layer_2_c)-2][position],M_position,l) / scaling_factor\n",
    "\n",
    "        x = layer_2_index_c\n",
    "        x2 = innerProd(x,x,M_position,l) / scaling_factor\n",
    "        x3 = innerProd(x,x2,M_position,l) / scaling_factor\n",
    "        x5 = innerProd(x3,x2,M_position,l) / scaling_factor\n",
    "        x7 = innerProd(x5,x2,M_position,l) / scaling_factor\n",
    "\n",
    "        xs = copy.deepcopy(v_onehot[5][0])\n",
    "        xs[1] = x[0]\n",
    "        xs[2] = x2[0]\n",
    "        xs[3] = x3[0]\n",
    "        xs[4] = x5[0]\n",
    "        xs[5] = x7[0]\n",
    "\n",
    "        out = mat_mul_forward(xs,H_sigmoid[0:1],scaling_factor)\n",
    "        out_rows.append(out)\n",
    "    return transpose(out_rows)[0]\n",
    "\n",
    "def load_linear_transformation(syn0_text,scaling_factor = 1000):\n",
    "    syn0_text *= scaling_factor\n",
    "    return linearTransformClient(syn0_text.T,getSecretKey(T_keys[len(syn0_text)-1]),T_keys[len(syn0_text)-1],l)\n",
    "\n",
    "def outer_product(x,y):\n",
    "    flip = False\n",
    "    if(len(x) < len(y)):\n",
    "        flip = True\n",
    "        tmp = x\n",
    "        x = y\n",
    "        y = tmp\n",
    "        \n",
    "    y_matrix = list()\n",
    "\n",
    "    for i in range(len(x)-1):\n",
    "        y_matrix.append(y)\n",
    "\n",
    "    y_matrix_transpose = transpose(y_matrix)\n",
    "\n",
    "    outer_result = list()\n",
    "    for i in range(len(x)-1):\n",
    "        outer_result.append(mat_mul_forward(x * onehot[len(x)-1][i],y_matrix_transpose,scaling_factor))\n",
    "    \n",
    "    if(flip):\n",
    "        return transpose(outer_result)\n",
    "    \n",
    "    return outer_result\n",
    "\n",
    "def mat_mul_forward(layer_1,syn1,scaling_factor):\n",
    "    \n",
    "    input_dim = len(layer_1)\n",
    "    output_dim = len(syn1)\n",
    "\n",
    "    buff = np.zeros(max(output_dim+1,input_dim+1))\n",
    "    buff[0:len(layer_1)] = layer_1\n",
    "    layer_1_c = buff\n",
    "    \n",
    "    syn1_c = list()\n",
    "    for i in range(len(syn1)):\n",
    "        buff = np.zeros(max(output_dim+1,input_dim+1))\n",
    "        buff[0:len(syn1[i])] = syn1[i]\n",
    "        syn1_c.append(buff)\n",
    "    \n",
    "    layer_2 = innerProd(syn1_c[0],layer_1_c,M_onehot[len(layer_1_c) - 2][0],l) / float(scaling_factor)\n",
    "    for i in range(len(syn1)-1):\n",
    "        layer_2 += innerProd(syn1_c[i+1],layer_1_c,M_onehot[len(layer_1_c) - 2][i+1],l) / float(scaling_factor)\n",
    "    return layer_2[0:output_dim+1]\n",
    "\n",
    "def elementwise_vector_mult(x,y,scaling_factor):\n",
    "    \n",
    "    y =[y]\n",
    "    \n",
    "    one_minus_layer_1 = transpose(y)\n",
    "\n",
    "    outer_result = list()\n",
    "    for i in range(len(x)-1):\n",
    "        outer_result.append(mat_mul_forward(x * onehot[len(x)-1][i],y,scaling_factor))\n",
    "        \n",
    "    return transpose(outer_result)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b16912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Let's tweak our network from before to model these phenomena\n",
    "class SentimentNetwork:\n",
    "    def __init__(self, reviews,labels,min_count = 10,polarity_cutoff = 0.1,hidden_nodes = 8, learning_rate = 0.1):\n",
    "       \n",
    "        np.random.seed(1234)\n",
    "    \n",
    "        self.pre_process_data(reviews, polarity_cutoff, min_count)\n",
    "        \n",
    "        self.init_network(len(self.review_vocab),hidden_nodes, 1, learning_rate)\n",
    "        \n",
    "        \n",
    "    def pre_process_data(self,reviews, polarity_cutoff,min_count):\n",
    "        \n",
    "        print(\"Pre-processing data...\")\n",
    "        \n",
    "        positive_counts = Counter()\n",
    "        negative_counts = Counter()\n",
    "        total_counts = Counter()\n",
    "\n",
    "        for i in range(len(reviews)):\n",
    "            if(labels[i] == 'POSITIVE'):\n",
    "                for word in reviews[i].split(\" \"):\n",
    "                    positive_counts[word] += 1\n",
    "                    total_counts[word] += 1\n",
    "            else:\n",
    "                for word in reviews[i].split(\" \"):\n",
    "                    negative_counts[word] += 1\n",
    "                    total_counts[word] += 1\n",
    "\n",
    "        pos_neg_ratios = Counter()\n",
    "\n",
    "        for term,cnt in list(total_counts.most_common()):\n",
    "            if(cnt >= 50):\n",
    "                pos_neg_ratio = positive_counts[term] / float(negative_counts[term]+1)\n",
    "                pos_neg_ratios[term] = pos_neg_ratio\n",
    "\n",
    "        for word,ratio in pos_neg_ratios.most_common():\n",
    "            if(ratio > 1):\n",
    "                pos_neg_ratios[word] = np.log(ratio)\n",
    "            else:\n",
    "                pos_neg_ratios[word] = -np.log((1 / (ratio + 0.01)))\n",
    "        \n",
    "        review_vocab = set()\n",
    "        for review in reviews:\n",
    "            for word in review.split(\" \"):\n",
    "                if(total_counts[word] > min_count):\n",
    "                    if(word in pos_neg_ratios.keys()):\n",
    "                        if((pos_neg_ratios[word] >= polarity_cutoff) or (pos_neg_ratios[word] <= -polarity_cutoff)):\n",
    "                            review_vocab.add(word)\n",
    "                    else:\n",
    "                        review_vocab.add(word)\n",
    "        self.review_vocab = list(review_vocab)\n",
    "        \n",
    "        label_vocab = set()\n",
    "        for label in labels:\n",
    "            label_vocab.add(label)\n",
    "        \n",
    "        self.label_vocab = list(label_vocab)\n",
    "        \n",
    "        self.review_vocab_size = len(self.review_vocab)\n",
    "        self.label_vocab_size = len(self.label_vocab)\n",
    "        \n",
    "        self.word2index = {}\n",
    "        for i, word in enumerate(self.review_vocab):\n",
    "            self.word2index[word] = i\n",
    "        \n",
    "        self.label2index = {}\n",
    "        for i, label in enumerate(self.label_vocab):\n",
    "            self.label2index[label] = i\n",
    "         \n",
    "        \n",
    "    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        # Set number of nodes in input, hidden and output layers.\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        print(\"Initializing Weights...\")\n",
    "        self.weights_0_1_t = np.zeros((self.input_nodes,self.hidden_nodes))\n",
    "    \n",
    "        self.weights_1_2_t = np.random.normal(0.0, self.output_nodes**-0.5, \n",
    "                                                (self.hidden_nodes, self.output_nodes))\n",
    "        \n",
    "        print(\"Encrypting Weights...\")\n",
    "        self.weights_0_1 = list()\n",
    "        for i,row in enumerate(self.weights_0_1_t):\n",
    "            sys.stdout.write(\"\\rEncrypting Weights from Layer 0 to Layer 1:\" + str(float((i+1) * 100) / len(self.weights_0_1_t))[0:4] + \"% done\")\n",
    "            self.weights_0_1.append(one_way_encrypt_vector(row,scaling_factor).astype('int64'))\n",
    "        print(\"\")\n",
    "        \n",
    "        self.weights_1_2 = list()\n",
    "        for i,row in enumerate(self.weights_1_2_t):\n",
    "            sys.stdout.write(\"\\rEncrypting Weights from Layer 1 to Layer 2:\" + str(float((i+1) * 100) / len(self.weights_1_2_t))[0:4] + \"% done\")\n",
    "            self.weights_1_2.append(one_way_encrypt_vector(row,scaling_factor).astype('int64'))           \n",
    "        self.weights_1_2 = transpose(self.weights_1_2)\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.layer_0 = np.zeros((1,input_nodes))\n",
    "        self.layer_1 = np.zeros((1,hidden_nodes))\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    \n",
    "    def sigmoid_output_2_derivative(self,output):\n",
    "        return output * (1 - output)\n",
    "    \n",
    "    def update_input_layer(self,review):\n",
    "\n",
    "        # clear out previous state, reset the layer to be all 0s\n",
    "        self.layer_0 *= 0\n",
    "        for word in review.split(\" \"):\n",
    "            self.layer_0[0][self.word2index[word]] = 1\n",
    "\n",
    "    def get_target_for_label(self,label):\n",
    "        if(label == 'POSITIVE'):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def train(self, training_reviews_raw, training_labels):\n",
    "\n",
    "        training_reviews = list()\n",
    "        for review in training_reviews_raw:\n",
    "            indices = set()\n",
    "            for word in review.split(\" \"):\n",
    "                if(word in self.word2index.keys()):\n",
    "                    indices.add(self.word2index[word])\n",
    "            training_reviews.append(list(indices))\n",
    "\n",
    "        layer_1 = np.zeros_like(self.weights_0_1[0])\n",
    "\n",
    "        start = time.time()\n",
    "        correct_so_far = 0\n",
    "        total_pred = 0.5\n",
    "        for i in range(len(training_reviews_raw)):\n",
    "            review_indices = training_reviews[i]\n",
    "            label = training_labels[i]\n",
    "\n",
    "            layer_1 *= 0\n",
    "            for index in review_indices:\n",
    "                layer_1 += self.weights_0_1[index]\n",
    "            layer_1 = layer_1 / float(len(review_indices))\n",
    "            layer_1 = layer_1.astype('int64') # round to nearest integer\n",
    "\n",
    "            layer_2 = sigmoid(innerProd(layer_1,self.weights_1_2[0],M_onehot[len(layer_1) - 2][1],l) / float(scaling_factor))[0:2]\n",
    "\n",
    "            if(label == 'POSITIVE'):\n",
    "                layer_2_delta = layer_2 - (c_ones[len(layer_2) - 2] * scaling_factor)\n",
    "            else:\n",
    "                layer_2_delta = layer_2\n",
    "\n",
    "            weights_1_2_trans = transpose(self.weights_1_2)\n",
    "            layer_1_delta = mat_mul_forward(layer_2_delta,weights_1_2_trans,scaling_factor).astype('int64')\n",
    "\n",
    "            self.weights_1_2 -= np.array(outer_product(layer_2_delta,layer_1))  * self.learning_rate\n",
    "\n",
    "            for index in review_indices:\n",
    "                self.weights_0_1[index] -= (layer_1_delta * self.learning_rate).astype('int64')\n",
    "\n",
    "            # we're going to decrypt on the fly so we can watch what's happening\n",
    "            total_pred += (s_decrypt(layer_2)[0] / scaling_factor)\n",
    "            if((s_decrypt(layer_2)[0] / scaling_factor) >= (total_pred / float(i+2)) and label == 'POSITIVE'):\n",
    "                correct_so_far += 1\n",
    "            if((s_decrypt(layer_2)[0] / scaling_factor) < (total_pred / float(i+2)) and label == 'NEGATIVE'):\n",
    "                correct_so_far += 1\n",
    "\n",
    "            reviews_per_second = i / float(time.time() - start)\n",
    "\n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(training_reviews_raw)))[:4] + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4] + \"%\")\n",
    "            if(i % 100 == 0):\n",
    "                print(i)\n",
    "\n",
    "    \n",
    "    def test(self, testing_reviews, testing_labels):\n",
    "        \n",
    "        correct = 0\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        for i in range(len(testing_reviews)):\n",
    "            pred = self.run(testing_reviews[i])\n",
    "            if(pred == testing_labels[i]):\n",
    "                correct += 1\n",
    "            \n",
    "            reviews_per_second = i / float(time.time() - start)\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(testing_reviews)))[:4] \\\n",
    "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                            + \"% #Correct:\" + str(correct) + \" #Tested:\" + str(i+1) + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\")\n",
    "    \n",
    "    def run(self, review):\n",
    "        \n",
    "        # Input Layer\n",
    "\n",
    "\n",
    "        # Hidden layer\n",
    "        self.layer_1 *= 0\n",
    "        unique_indices = set()\n",
    "        for word in review.lower().split(\" \"):\n",
    "            if word in self.word2index.keys():\n",
    "                unique_indices.add(self.word2index[word])\n",
    "        for index in unique_indices:\n",
    "            self.layer_1 += self.weights_0_1[index]\n",
    "        \n",
    "        # Output layer\n",
    "        layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2))\n",
    "        \n",
    "        if(layer_2[0] >= 0.5):\n",
    "            return \"POSITIVE\"\n",
    "        else:\n",
    "            return \"NEGATIVE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "714eab2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-a04a27867abf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentimentNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpolarity_cutoff\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'reviews' is not defined"
     ]
    }
   ],
   "source": [
    "mlp = SentimentNetwork(reviews[:-1000],labels[:-1000],min_count=20,polarity_cutoff=0.15,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cec521ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-72b1015ac551>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msaved_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights_0_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights_1_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'mlp' is not defined"
     ]
    }
   ],
   "source": [
    "saved_weights = (copy.deepcopy(mlp.weights_0_1),copy.deepcopy(mlp.weights_1_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "365a1455",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'saved_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-c5f4cf8196bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights_0_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaved_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights_1_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaved_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0125\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'saved_weights' is not defined"
     ]
    }
   ],
   "source": [
    "mlp.weights_0_1 = copy.deepcopy(saved_weights[0])\n",
    "mlp.weights_1_2 = copy.deepcopy(saved_weights[1])\n",
    "mlp.learning_rate = 0.0125\n",
    "mlp.train(reviews[0:-1000],labels[:-1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f854ca15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.5699)\n",
      "tensor(14.5132)\n",
      "tensor(11.8150)\n",
      "tensor(0.9228)\n",
      "tensor(0.8528)\n",
      "tensor(0.7952)\n",
      "tensor(0.7251)\n",
      "tensor(0.6415)\n",
      "tensor(0.5468)\n",
      "tensor(0.4463)\n",
      "tensor(0.3658)\n",
      "tensor(0.2903)\n",
      "tensor(0.2211)\n",
      "tensor(0.1671)\n",
      "tensor(0.1203)\n",
      "tensor(0.0894)\n",
      "tensor(0.0622)\n",
      "tensor(0.0462)\n",
      "tensor(0.0339)\n",
      "tensor(0.0251)\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# A Toy Dataset\n",
    "data = th.tensor([[0,0],[0,1],[1,0],[1,1.]], requires_grad=True)\n",
    "target = th.tensor([[0],[0],[1],[1.]], requires_grad=True)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 20)\n",
    "        self.fc2 = nn.Linear(20, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# A Toy Model\n",
    "model = Net()\n",
    "\n",
    "def train():\n",
    "    # Training Logic\n",
    "    opt = optim.SGD(params=model.parameters(),lr=0.1)\n",
    "    for iter in range(20):\n",
    "\n",
    "        # 1) erase previous gradients (if they exist)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # 2) make a prediction\n",
    "        pred = model(data)\n",
    "\n",
    "        # 3) calculate how much we missed\n",
    "        loss = ((pred - target)**2).sum()\n",
    "\n",
    "        # 4) figure out which weights caused us to miss\n",
    "        loss.backward()\n",
    "\n",
    "        # 5) change those weights\n",
    "        opt.step()\n",
    "\n",
    "        # 6) print our progress\n",
    "        print(loss.data)\n",
    "        \n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4efb168d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0840],\n",
       "        [-0.0099],\n",
       "        [ 0.9791],\n",
       "        [ 0.8927]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b094b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting syft\n",
      "  Using cached syft-0.5.0-py2.py3-none-any.whl (504 kB)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\reena\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\reena\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'c:\\\\users\\\\reena\\\\anaconda3\\\\lib\\\\site-packages\\\\torch-1.4.0.dist-info\\\\METADATA'\n",
      "\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\reena\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\reena\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\reena\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: packaging in c:\\users\\reena\\anaconda3\\lib\\site-packages (from syft) (20.9)\n",
      "Requirement already satisfied: PyNaCl in c:\\users\\reena\\anaconda3\\lib\\site-packages (from syft) (1.4.0)\n",
      "Collecting PyJWT==1.7.1\n",
      "  Using cached PyJWT-1.7.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting requests-toolbelt\n",
      "  Using cached requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "Collecting names\n",
      "  Using cached names-0.3.0-py3-none-any.whl\n",
      "Requirement already satisfied: protobuf in c:\\users\\reena\\anaconda3\\lib\\site-packages (from syft) (3.17.0)\n",
      "Requirement already satisfied: requests in c:\\users\\reena\\anaconda3\\lib\\site-packages (from syft) (2.25.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\reena\\anaconda3\\lib\\site-packages (from syft) (3.7.4.3)\n",
      "Collecting aiortc\n",
      "  Using cached aiortc-1.2.0-cp38-cp38-win_amd64.whl (978 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\reena\\anaconda3\\lib\\site-packages (from syft) (1.2.4)\n",
      "Requirement already satisfied: torch<=1.8.1,>=1.4.0 in c:\\users\\reena\\anaconda3\\lib\\site-packages (from syft) (1.4.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\reena\\anaconda3\\lib\\site-packages (from syft) (1.12.1)\n",
      "Collecting forbiddenfruit>=0.1.3\n",
      "  Using cached forbiddenfruit-0.1.4-py3-none-any.whl\n",
      "Requirement already satisfied: cryptography>=3.4.7 in c:\\users\\reena\\anaconda3\\lib\\site-packages (from syft) (3.4.7)\n",
      "Collecting pyarrow\n",
      "  Using cached pyarrow-4.0.1-cp38-cp38-win_amd64.whl (13.3 MB)\n",
      "Requirement already satisfied: flask<2.0.0,>=1.1.2 in c:\\users\\reena\\anaconda3\\lib\\site-packages (from syft) (1.1.2)\n",
      "Collecting dpcontracts\n",
      "  Using cached dpcontracts-0.6.0-py3-none-any.whl\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\reena\\anaconda3\\lib\\site-packages (from syft) (5.4.1)\n",
      "Collecting torchcsprng<=0.2.1\n",
      "  Using cached torchcsprng-0.2.1-cp38-cp38-win_amd64.whl (1.3 MB)\n",
      "Collecting sqlitedict\n",
      "  Using cached sqlitedict-1.7.0-py3-none-any.whl\n",
      "Collecting websocket-client\n",
      "  Using cached websocket_client-1.1.0-py2.py3-none-any.whl (68 kB)\n",
      "Requirement already satisfied: torchvision<=0.9.1,>=0.5 in c:\\users\\reena\\anaconda3\\lib\\site-packages (from syft) (0.5.0)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\reena\\anaconda3\\lib\\site-packages (from syft) (1.5.1)\n",
      "Collecting loguru\n",
      "  Using cached loguru-0.5.3-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\reena\\anaconda3\\lib\\site-packages (from syft) (0.24.1)\n",
      "Collecting syft-proto\n",
      "  Using cached syft_proto-0.5.3-py3-none-any.whl (66 kB)\n",
      "Requirement already satisfied: cachetools in c:\\users\\reena\\anaconda3\\lib\\site-packages (from syft) (4.2.2)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\reena\\anaconda3\\lib\\site-packages (from cryptography>=3.4.7->syft) (1.14.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\reena\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=3.4.7->syft) (2.20)\n",
      "Requirement already satisfied: click>=5.1 in c:\\users\\reena\\anaconda3\\lib\\site-packages (from flask<2.0.0,>=1.1.2->syft) (7.1.2)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in c:\\users\\reena\\anaconda3\\lib\\site-packages (from flask<2.0.0,>=1.1.2->syft) (2.11.3)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in c:\\users\\reena\\anaconda3\\lib\\site-packages (from flask<2.0.0,>=1.1.2->syft) (1.0.1)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in c:\\users\\reena\\anaconda3\\lib\\site-packages (from flask<2.0.0,>=1.1.2->syft) (1.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\reena\\anaconda3\\lib\\site-packages (from Jinja2>=2.10.1->flask<2.0.0,>=1.1.2->syft) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install syft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaadd404",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'syft'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-f57fb72cdc59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0msyft\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#import torch as th\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTorchHook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#from torch import nn, optim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'syft'"
     ]
    }
   ],
   "source": [
    "import syft as sy\n",
    "#import torch as th\n",
    "hook = sy.TorchHook(th)\n",
    "#from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e13b8a12",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Net' object has no attribute 'fix_precision'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-98c145437865>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mencrypted_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfix_precision\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcrypto_provider\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msecure_worker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    573\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[0;32m    576\u001b[0m             type(self).__name__, name))\n\u001b[0;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Net' object has no attribute 'fix_precision'"
     ]
    }
   ],
   "source": [
    "encrypted_model = model.fix_precision().share(alice, bob, crypto_provider=secure_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2092e310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
